#!/usr/bin/env python3
"""
DiagnÃ³stico do workflow automÃ¡tico
"""

import json
import sys
from pathlib import Path
from datetime import datetime

def print_header(title):
    """Printa um header formatado"""
    print(f"\n{'=' * 70}")
    print(f"ğŸ“‹ {title}")
    print(f"{'=' * 70}\n")

def check_files():
    """Verifica arquivos necessÃ¡rios"""
    print_header("VERIFICAÃ‡ÃƒO DE ARQUIVOS")
    
    files = {
        'scraper.py': 'ğŸ•·ï¸  Script de scraping',
        'requirements.txt': 'ğŸ“¦ DependÃªncias Python',
        'data/': 'ğŸ’¾ Pasta de dados',
        '.github/workflows/check-consultas.yml': 'âš™ï¸  Workflow GitHub Actions',
        'docs/index.html': 'ğŸ“„ PÃ¡gina HTML',
    }
    
    all_ok = True
    for file, label in files.items():
        exists = Path(file).exists()
        status = 'âœ…' if exists else 'âŒ'
        print(f"{status} {label:30} â†’ {file}")
        if not exists:
            all_ok = False
    
    return all_ok

def check_scraper():
    """Verifica conteÃºdo do scraper.py"""
    print_header("VERIFICAÃ‡ÃƒO DO SCRAPER.PY")
    
    if not Path('scraper.py').exists():
        print("âŒ scraper.py nÃ£o encontrado!")
        return False
    
    with open('scraper.py', 'r', encoding='utf-8') as f:
        content = f.read()
    
    checks = {
        'def ': 'ContÃ©m funÃ§Ãµes',
        'requests': 'Importa requests',
        'BeautifulSoup': 'Importa BeautifulSoup',
        'selenium': 'Importa Selenium',
        'if __name__': 'Tem bloco main',
        'data/consultas.json': 'Salva em data/consultas.json',
    }
    
    all_ok = True
    for check, label in checks.items():
        found = check in content
        status = 'âœ…' if found else 'âš ï¸ '
        print(f"{status} {label:30} â†’ {check}")
        if not found and check != 'requests':  # requests Ã© opcional
            all_ok = False
    
    # Tamanho do arquivo
    lines = len(content.split('\n'))
    print(f"\nğŸ“Š Tamanho: {len(content)} caracteres, {lines} linhas")
    
    return all_ok

def check_requirements():
    """Verifica requirements.txt"""
    print_header("VERIFICAÃ‡ÃƒO DE DEPENDÃŠNCIAS")
    
    if not Path('requirements.txt').exists():
        print("âŒ requirements.txt nÃ£o encontrado!")
        return False
    
    with open('requirements.txt', 'r') as f:
        reqs = f.read()
    
    required = {
        'selenium': 'Scraping com JavaScript',
        'beautifulsoup4': 'Parse HTML',
        'requests': 'HTTP requests',
        'webdriver-manager': 'Gerenciar ChromeDriver',
    }
    
    all_ok = True
    for pkg, desc in required.items():
        found = pkg in reqs
        status = 'âœ…' if found else 'âŒ'
        print(f"{status} {pkg:20} â†’ {desc}")
        if not found:
            all_ok = False
    
    # Mostra conteÃºdo
    print(f"\nğŸ“ ConteÃºdo de requirements.txt:")
    print("â”€" * 70)
    print(reqs)
    print("â”€" * 70)
    
    return all_ok

def check_workflow():
    """Verifica workflow YAML"""
    print_header("VERIFICAÃ‡ÃƒO DO WORKFLOW")
    
    workflow_file = Path('.github/workflows/check-consultas.yml')
    
    if not workflow_file.exists():
        print("âŒ Workflow nÃ£o encontrado em .github/workflows/check-consultas.yml")
        return False
    
    with open(workflow_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    checks = {
        'schedule:': 'Tem agendamento (cron)',
        'cron:': 'ConfiguraÃ§Ã£o cron presente',
        'python scraper.py': 'Executa scraper.py',
        'Setup Python': 'Configura Python',
        'requirements.txt': 'Instala dependÃªncias',
        'workflow_dispatch': 'Permite execuÃ§Ã£o manual',
    }
    
    all_ok = True
    for check, label in checks.items():
        found = check in content
        status = 'âœ…' if found else 'âŒ'
        print(f"{status} {label:30} â†’ {check}")
        if not found:
            all_ok = False
    
    # Mostra parte relevante
    if 'schedule:' in content:
        print(f"\nğŸ“… HorÃ¡rios de execuÃ§Ã£o configurados:")
        for line in content.split('\n'):
            if 'cron:' in line:
                print(f"   {line.strip()}")
    
    return all_ok

def check_data():
    """Verifica arquivo de dados"""
    print_header("VERIFICAÃ‡ÃƒO DE DADOS")
    
    data_file = Path('data/consultas.json')
    
    if not data_file.exists():
        print("âš ï¸  data/consultas.json nÃ£o encontrado (serÃ¡ criado na primeira execuÃ§Ã£o)")
        return True
    
    try:
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"âœ… JSON vÃ¡lido")
        print(f"   NÃºmero de consultas: {len(data.get('consultas', []))}")
        print(f"   Ãšltima atualizaÃ§Ã£o: {data.get('ultimaAtualizacao', 'N/A')}")
        
        if data.get('consultas'):
            print(f"\nğŸ“Š Amostra de dados:")
            for i, c in enumerate(data['consultas'][:2], 1):
                print(f"\n   Consulta {i}:")
                print(f"     ID: {c.get('id')}")
                print(f"     TÃ­tulo: {c.get('titulo', '')[:50]}...")
                print(f"     Dias restantes: {c.get('dias_restantes')}")
        
        return True
    except Exception as e:
        print(f"âŒ Erro ao ler JSON: {e}")
        return False

def run_scraper_test():
    """Testa se o scraper pode ser importado"""
    print_header("TESTE DE SCRAPER")
    
    try:
        # Tenta importar
        import sys
        sys.path.insert(0, str(Path.cwd()))
        
        # Verifica sintaxe
        with open('scraper.py', 'r', encoding='utf-8') as f:
            compile(f.read(), 'scraper.py', 'exec')
        
        print("âœ… scraper.py tem sintaxe vÃ¡lida (Python)")
        return True
    except SyntaxError as e:
        print(f"âŒ Erro de sintaxe em scraper.py:")
        print(f"   Linha {e.lineno}: {e.msg}")
        return False
    except Exception as e:
        print(f"âš ï¸  NÃ£o foi possÃ­vel verificar: {e}")
        return True

def main():
    print("\n" + "â•”" + "=" * 68 + "â•—")
    print("â•‘" + " " * 68 + "â•‘")
    print("â•‘" + "  ğŸ” DIAGNÃ“STICO - WORKFLOW AUTOMÃTICO DE SCRAPING".center(68) + "â•‘")
    print("â•‘" + " " * 68 + "â•‘")
    print("â•š" + "=" * 68 + "â•")
    
    results = {}
    
    results['files'] = check_files()
    results['scraper'] = check_scraper()
    results['requirements'] = check_requirements()
    results['workflow'] = check_workflow()
    results['data'] = check_data()
    results['syntax'] = run_scraper_test()
    
    # Resumo
    print_header("RESUMO")
    
    all_ok = all(results.values())
    
    status_map = {
        'files': 'Arquivos necessÃ¡rios',
        'scraper': 'ConteÃºdo do scraper.py',
        'requirements': 'DependÃªncias',
        'workflow': 'ConfiguraÃ§Ã£o GitHub Actions',
        'data': 'Arquivo de dados',
        'syntax': 'Sintaxe Python',
    }
    
    for key, label in status_map.items():
        status = 'âœ…' if results[key] else 'âŒ'
        print(f"{status} {label}")
    
    print("\n" + "=" * 70)
    
    if all_ok:
        print("âœ… TUDO OK! Seu workflow estÃ¡ pronto para usar.")
        print("\nğŸ“ PrÃ³ximos passos:")
        print("   1. git add .")
        print("   2. git commit -m 'fix: workflow automÃ¡tico'")
        print("   3. git push origin main")
        print("\n   Depois de 5 minutos, vÃ¡ para:")
        print("   https://github.com/brenoeng/bot_consultas_publicas/actions")
        return 0
    else:
        print("âš ï¸  PROBLEMAS DETECTADOS - Veja acima os itens com âŒ")
        return 1

if __name__ == '__main__':
    sys.exit(main())
