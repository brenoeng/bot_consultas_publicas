# ğŸ•·ï¸ Guia do Scraper - Consultas PÃºblicas

## O que Ã©?

Script Python que **automaticamente**:

- âœ… Acessa https://consultas-publicas.mme.gov.br/home
- âœ… Extrai dados das consultas pÃºblicas
- âœ… Salva em `data/consultas.json`
- âœ… Atualiza as pÃ¡ginas HTML automaticamente
- âœ… Roda em GitHub Actions (agendado 3x por dia)

## ğŸš€ Como Usar

### 1ï¸âƒ£ Setup Local (Desenvolvimento)

```bash
# Instalar dependÃªncias
pip install -r requirements.txt

# Ou usar setup automÃ¡tico
python setup_scraper.py

# Testar scraper
python scraper.py
```

### 2ï¸âƒ£ Usar GitHub Actions (AutomÃ¡tico em ProduÃ§Ã£o)

1. **Commit seu cÃ³digo:**

   ```bash
   git add scraper.py requirements.txt
   git commit -m "Add scraper automÃ¡tico"
   git push origin main
   ```

2. **Ativa automaticamente:**

   - âœ… Todos os dias Ã s 08:00, 12:00 e 18:00 UTC
   - âœ… Manual: Settings â†’ Actions â†’ Workflows â†’ Run

3. **Resultado:**
   - ğŸ“ Dados salvos em `data/consultas.json`
   - ğŸ”„ Commit automÃ¡tico se houver mudanÃ§as
   - ğŸ“„ GitHub Pages atualiza automaticamente

## ğŸ“Š Estrutura de Dados

O scraper gera `data/consultas.json` com este formato:

```json
{
  "consultas": [
    {
      "id": "consulta_unique_id_hash",
      "titulo": "Consulta PÃºblica nÂº XX/2025 - TÃ­tulo",
      "descricao": "DescriÃ§Ã£o da consulta",
      "data_abertura": "2025-11-26",
      "data_encerramento": "2025-12-10",
      "url_oficial": "https://consultas-publicas.mme.gov.br/...",
      "dias_restantes": 14,
      "notificado": false
    }
  ],
  "ultimaAtualizacao": "2025-11-26T12:34:56.789Z"
}
```

## ğŸ”§ ConfiguraÃ§Ã£o

### FrequÃªncia de ExecuÃ§Ã£o

Edite `.github/workflows/check-consultas.yml`:

```yaml
schedule:
  - cron: "0 8 * * *" # 08:00 UTC
  - cron: "0 12 * * *" # 12:00 UTC
  - cron: "0 18 * * *" # 18:00 UTC
```

[Gerador de cron](https://crontab.guru)

### VariÃ¡veis de Ambiente

```bash
# NÃ£o necessÃ¡rio atualmente, mas vocÃª pode adicionar:
export LOG_LEVEL=INFO
export REQUEST_TIMEOUT=15
export MAX_RETRIES=3
```

## ğŸ“Š Monitoramento

### Ver Logs Local

```bash
# Enquanto roda
python scraper.py

# Arquivo de log
tail -f scraper.log
```

### Ver Logs GitHub Actions

1. VÃ¡ para seu repositÃ³rio
2. Aba "Actions"
3. Clique no workflow "Scraper - Consultas PÃºblicas"
4. Clique na execuÃ§Ã£o
5. Veja os logs detalhados

## ğŸ› Troubleshooting

### âŒ "Site nÃ£o carrega"

**Problema:** Timeout ao acessar site
**SoluÃ§Ã£o:** Site pode estar offline ou mudou de estrutura

```bash
# Teste manualmente
python scraper.py

# Verifique logs
cat scraper.log
```

### âŒ "Nenhuma consulta encontrada"

**Problema:** HTML do site mudou
**SoluÃ§Ã£o:** Adaptar seletores CSS em `scraper.py`

```python
# Em scraper.py, linha ~180
# Ajuste os seletores:
seletores = [
    ('div', {'class': lambda x: x and 'consulta' in x}),
    ('div', {'class': lambda x: x and 'card' in x}),
    # Adicione novos aqui
]
```

### âŒ "JSON invÃ¡lido"

**Problema:** Dados salvos estÃ£o mal formatados
**SoluÃ§Ã£o:**

```bash
# Validar JSON
python -m json.tool data/consultas.json

# Se falhar, restaurar backup
git restore data/consultas.json
```

## ğŸ¯ CustomizaÃ§Ãµes

### Adicionar ValidaÃ§Ã£o Extra

```python
# Em scraper.py, mÃ©todo validar_consulta()
def validar_consulta(self, consulta):
    # ... validaÃ§Ãµes existentes ...

    # Nova validaÃ§Ã£o
    if len(consulta['descricao']) < 10:
        logger.warning("DescriÃ§Ã£o muito curta")
        return False

    return True
```

### Enviar NotificaÃ§Ã£o WhatsApp

```python
# Descomentar em notifier.py
# E configurar secrets no GitHub:
# - TWILIO_ACCOUNT_SID
# - TWILIO_AUTH_TOKEN
# - TWILIO_PHONE_NUMBER
```

### Salvar HistÃ³rico

```python
# Adicionar em scraper.py
import shutil
from datetime import datetime

# Backup antes de salvar
backup_name = f"data/backup_consultas_{datetime.now().isoformat()}.json"
shutil.copy(self.data_file, backup_name)
```

## ğŸ“ˆ MÃ©tricas

**Tempo de execuÃ§Ã£o:**

- Local: ~5-15 segundos
- GitHub Actions: ~30-60 segundos (incluindo setup)

**Taxa de sucesso:**

- Esperado: >95%
- Falha: Usa dados anteriores automaticamente

**Tamanho de dados:**

- Arquivo JSON: ~5-50 KB
- Consultas: 10-100 por execuÃ§Ã£o

## ğŸ”„ Pipeline AutomÃ¡tico

```
GitHub Actions Trigger (3x/dia)
         â†“
   Checkout cÃ³digo
         â†“
   Setup Python 3.11
         â†“
   Instalar dependÃªncias (pip)
         â†“
   Executar scraper.py
         â†“
   Validar JSON
         â†“
   Commit mudanÃ§as (se houver)
         â†“
   Push para main
         â†“
   Deploy GitHub Pages
         â†“
   âœ“ PÃ¡ginas atualizadas
```

## ğŸ“ Logging

Scraper cria dois logs:

1. **Console:** SaÃ­da em tempo real
2. **Arquivo:** `scraper.log` (sempre presente)

Formato:

```
2025-11-26 12:34:56,789 - INFO - Iniciando scraper...
2025-11-26 12:34:57,890 - INFO - âœ“ PÃ¡gina carregada com sucesso
2025-11-26 12:34:58,901 - INFO - âœ“ Consulta: TÃ­tulo da Consulta...
2025-11-26 12:35:00,012 - INFO - âœ“ Dados salvos em data/consultas.json
```

## âœ… Checklist

- [ ] `requirements.txt` criado
- [ ] `scraper.py` funcionando localmente
- [ ] GitHub Actions workflow ativo
- [ ] `data/consultas.json` populado
- [ ] PÃ¡ginas HTML exibem dados
- [ ] Log `scraper.log` sendo criado

## ğŸ“ Suporte

Se algo quebrar:

1. Verifique `scraper.log`
2. Teste localmente: `python scraper.py`
3. Verifique logs do GitHub Actions
4. Valide JSON: `python -m json.tool data/consultas.json`

---

**Criado:** Novembro 2025
**Status:** âœ… Pronto para ProduÃ§Ã£o
